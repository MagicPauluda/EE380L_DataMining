{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 85 </p>\n",
    "## <p style=\"text-align: center;\">Due: September 29th(09/29/2020) submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.\n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)*\n",
    "\n",
    "For the descriptive questions, you can write down the solution in paper and embed a picture of it to the notebook or type it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Regularized regression (40 pts)\n",
    "The data given is in \"data.csv\" - this is same fish dataset from the previous assignment, in this question you will explore the application of Lasso and Ridge regression using sklearn package in Python.\n",
    "\n",
    "* Use the below code to load the dataset given in data.csv. Create a train_test split of 75:25 with random state = 50\n",
    "\n",
    "* Scale the data so that each of the independent variables have zero mean and unit variance. You can use the [sklearn.preprocessing.scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data.csv\", index_col=0)\n",
    "df = df[~df.isin([0, np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "X = df.drop(['Weight'], axis=1)\n",
    "y = df['Weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{10}$ to $\\lambda = 10^{-2}$. In Python, you can consider this range of values as follows:\n",
    "\n",
    "      import numpy as np\n",
    "\n",
    "      alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation. The cross validation should happen on your training data using  average MSE as the scoring metric. (8pts)\n",
    "\n",
    "b) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot. What do you qualitatively observe when value of the regularization parameter is changed? (7pts)\n",
    "\n",
    "c) Run least squares regression, ridge, and lasso on the training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error (MAE), Mean Squared Error(MSE) and ${R^2}$ on the test data for each. (5pts)\n",
    "\n",
    "d) Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "Report the best $\\lambda$ based on cross validation. Run lasso on the training data using the best $\\lambda$ and report the coefficeints for all variables. (5pts)\n",
    "\n",
    "e) Why did we have to scale the data before regularization? (5pts)\n",
    "\n",
    "f) Lasso and ridge regularization techniques are often used to combat overfitting during linear regression. Which of the two yields more sparse models (i.e. fewer number of parameters) when the tuning parameter $\\lambda$ is sufficiently large (but not infinite)? (5 pts)\n",
    "\n",
    "g) Run ElasticNet with the same values of alphas on the training data, and find the best value for alpha using MSE. Report the ${R^2}$ on test data and plot a graph showing the predictions and actual labels. Explain the results comparing the three regression models.(5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Problem - Linear Regression: Least Squares and MLE (Not to be submitted)  \n",
    "This question is a practise for you all to understand mathematically the assumptions of linear regression model. \n",
    "\n",
    "Consider linear regression with a single independent variable. Thus, predicted values are given by:  \n",
    "$$y  = w_0  + w_1 x$$\n",
    "\n",
    "Mathematically show that if the assumptions behind linear regression hold (slide 3 of the MLR slides), then the values of $w_0$ and $w_1$ obtained by minimizing MSE are indeed the  maximum likelihood solution of the corresponding underlying probability model relating the target variable to the input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Linear Regression: Regularizaton - Least Squares and MLE (5 pts)\n",
    "Consider linear regression with an additional regularization term. The, predicted values are given by:  \n",
    "$$\\textrm{min}_{\\beta} \\Bigg\\{\\sum_{i}(y_{i}-\\beta x_i{'})^2 \\Bigg\\} + \\lambda \\beta^2$$ \n",
    "\n",
    "\n",
    "where in this problem $y_i$, $x_i$, and $\\beta$ are all scalars. Find a formula for $\\beta$ that minimizes the equation above and prove that $\\beta$ minimizes the equation above. $\\lambda$ is a positive constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Finding ${R^2}$ (10 pts)\n",
    "\n",
    "Suppose we are given samples generated according to\n",
    "\n",
    "$y_i$ = $8x_i$ + $w_i$\n",
    "where $w_i$ and $x_i$ are $N(0,1)$. \n",
    "\n",
    "Assume we have infinite samples.\n",
    "\n",
    "(a) What will be the linear regression predictor $\\hat{\\beta}$ ? (5 pts)\n",
    "\n",
    "(b) What will be the ${R^2}$ of this $\\hat{\\beta}$  ? (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Collinearity in Multiple Linear Regression (5pts)\n",
    "What do you understand by the collinearity problem that can be encountered in MLR? Suggest one way of alleviating this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 - Bias - Variance (5 pts)\n",
    "Suppose you learn a ridge regression based on some of the independent variables available to you,\n",
    "and including a few interaction terms as well. Your result is disappointing, and you believe that the model is\n",
    "suffering from either too much bias or too much variance. Describe briefly how you will test which alternative is\n",
    "more likely to be true. (there is no extra data that you can use, so “get more data” is not an option).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 (20 pts): \n",
    "\n",
    "We want to build a model that can predict y for unknown inputs x.(15 pts)\n",
    "\n",
    "(a) (5 pts) Fit a linear model to the training data, and report mean squared error on the test data. Plot the data (y_train vs x_train and y_test vs x_test) and predictions on the test set (prediction on x_test vs x_test), clearly denoting the training, testing, and predicted points. All the plots must be in the same figure and be clearly labeled.\n",
    "\n",
    "(b) (10 pts) Fit polynomial models of degrees 2, 3, 4, 6 and 8 to the training data, Report mean squared error (on both train and test sets) for all the models. Plot the data (y_train vs x_train and y_test vs x_test), the fitted models (prediction on x_all by different models vs x_all), and the predictions on the test set (prediction on x_test by different models vs x_test). All the plots must be in the same figure and be clearly labeled.\n",
    "\n",
    "\n",
    "(c) (5 pts) Which model performed the best? Explain using the bias-variance tradeoff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the below code to load the data from the file 'ps01.data'. It is organized as a dictionary, of train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# code here\n",
    "\n",
    "data_load = np.load('ps01.data',allow_pickle=True)\n",
    "x_train = data_load.item().get(\"Xtrain\")\n",
    "y_train = data_load.item().get(\"Ytrain\")\n",
    "x_test = data_load.item().get(\"Xtest\")\n",
    "y_test =data_load.item().get(\"Ytest\")\n",
    "x_all = np.linspace(-10,10,101).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
